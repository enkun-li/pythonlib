#!/usr/bin/env python
#-*- coding: utf-8 -*-  
#==================================
# File Name: analyze_mcmc.pyx
# Author: ekli
# Mail: ekli_091@mail.dlut.edu.cn  
# Created Time: 2019-02-26 16:46:41
#==================================

import numpy as np
import pandas as pd
import os
import getdist
from getdist.mcsamples import MCSamples
from cobaya.yaml import yaml_load_file
from getdist.gaussian_mixtures import MixtureND

class Cobaya2MCMC:
    '''
    This function is used to load the sample chains generated by Cobaya:
    ========================================================================
    root: the file name's root
    burns: [default 0.3]
    re_weight: used for importance sampling, re_weight is -log(likelihood),
    its shape should be identical with the total sample results.
               [default None]
    name_tag: [default None]
    ignore_rows: [default 0]
    ini: [default None]
    settings: for MCSamples(settings=)
    '''

    def __init__(self, root, burns=0.3):
        self.root = root
        self.info = yaml_load_file(root+'__full.yaml')
        if not hasattr(self.info, "keys"):
            raise TypeError("Cannot regonise arguments. Are you sure you are calling "
                    "with (info, collections, ...) in that order?")
        #=============================================
        # read in header names
        hnames = pd.read_csv(root+'_1.txt', header=None, 
                sep='\s+', nrows=1)
        num = hnames.columns[-1]
        # the columns names
        self.columns = ['']*num
        for i in range(1,num+1):
            self.columns[i-1] = hnames[i][0]
        #-------------------------------------------
        # read in the data
        sp = root.rsplit(sep='/')[-1]
        isp = len(sp)
        dirs = root.rstrip(sp)
        test_dat = pd.read_csv(root+'_1.txt', header=None, 
                names=self.columns, sep='\s+')
        self.burn_rows = int(test_dat.shape[0]*burns)        

        dats = pd.DataFrame()
        for ifile in os.listdir(dirs):
            if(ifile[-4:] == '.txt' and ifile[:isp] == sp):
                print(dirs+ifile)
                dat = pd.read_csv(dirs+ifile, header=None, 
                        skiprows=self.burn_rows+1, 
                        names = self.columns, sep='\s+')
                dats = dats.append(dat, ignore_index=True)
        self.pp = dats
        #pp = pd.read_csv(root+'_1.txt', header=None, skiprows=1,
        #             names=columns, sep='\s+')
        return

    def sample_info(self):
        #==================================
        from getdist.yaml_format_tools import _p_label, _p_renames, _weight, _minuslogpost
        from getdist.yaml_format_tools import get_info_params, get_range, is_derived_param
        from getdist.yaml_format_tools import get_sampler_type
        #==================================
        # Check consistency with info
        columns = self.columns
        info_params = get_info_params(self.info)
        assert set(columns[2:]) == set(info_params.keys()), (
                "Info and collection(s) are not compatible, because their parameters differ: "
                "the collection(s) have %r and the info has %r. " % (
                    columns[2:], list(info_params.keys())) +
                "Are you sure that you are using an *updated* info dictionary "
                "(i.e. the output of `cobaya.run`)?")
        # We need to use *collection* sorting, not info sorting!
        self.names = [p + ("*" if is_derived_param(info_params[p]) else "")
                for p in columns[2:]]
        self.labels = [(info_params[p] or {}).get(_p_label, p) for p in columns[2:]]
        self.ranges = {p: get_range(info_params[p]) for p in columns[2:]}
        self.renames = {p: info_params.get(p, {}).get(_p_renames, []) for p in columns[2:]}
        self.samples = [np.array([self.pp[nn].values for nn in columns[2:]]).T]
        self.get_weights = [self.pp[nn].values for nn in columns[0:1]]
        self.loglikes = [-self.pp[nn].values for nn in columns[1:2]]
        self.sampler = get_sampler_type(self.info)
        return

    def __mcsample(self):
        samples = MCSamples(samples=self.samples, weights=self.weights, 
                loglikes=self.loglikes, sampler=self.sampler,
                names=self.names, labels=self.labels, ranges=self.ranges, 
                renames=self.renames)
        return samples

    def output_samples(self, re_weight=None):
        self.sample_info()
        self.weights = self.get_weights
        samples = self.__mcsample()
        return samples

    def importance_sampling(self, re_weight=None):
        self.sample_info()
        if(re_weight is None):
            self.weights = self.get_weights
        else:
            scale = np.min(re_weight)
            reweight_loglike = re_weight - scale
            self.weights = np.int64( self.get_weights*np.exp(-reweight_loglike) )
        sample = self.__mcsample()
        return sample

    #def general_mcsample(self):
    #    self.sample_info()
    #    num = np.shape(self.get_weights)[1]
    #    ss = {'samples':[[]], 'lnchisq': [[]]}
    #    ss['samples'] = self.samples[0]
    #    ss['lnchisq'] = self.loglikes[0]
    #    for i in range(num):
    #        n = self.get_weights[0][i]
    #        l = self.loglikes[0][i]
    #        s = self.samples[0][i]
    #        if(n>1):
    #            ll = np.array([l]*(n-1))
    #            sa = np.array([s]*(n-1))
    #            ss['lnchisq'] = np.append(ss['lnchisq'], ll, axis=0)
    #            ss['samples'] = np.append(ss['samples'], sa, axis=0)
    #        #print(n, end=' :  ')
    #        #print(l, end=' :: ')
    #        #print(np.array([l]*n))
    #    
    #    #self.weights = np.ones(num)
    #    #self.loglikes = self.get_weights*self.loglikes
    #    #sample = self.__mcsample()
    #    return ss

    def general_mcsample(self):
        '''
        Out put a dict contains: samples, loglikes, weights
        '''
        #cdef int num
        cdef dict ss
        ss = {'samples': [], 'loglikes': [], 'weights': []}
        self.sample_info()
        #num = np.shape(self.get_weights)[1]
        ss['samples'] = self.samples[0]
        ss['loglikes'] = self.loglikes[0]
        ss['weights'] = self.get_weights[0]
        return ss


#=================================================================================
def loadCobayaSamples_file(root,burns=0.3, re_weight=None, name_tag=None,\
        ignore_rows=0, ini=None, settings={}):
    '''
    This function is used to load the sample chains generated by Cobaya:
    ========================================================================
    root: the file name's root
    burns: [default 0.3]
    re_weight: used for importance sampling, re_weight is -log(likelihood),
               its shape should be identical with the total sample results.
               [default None]
    name_tag: [default None]
    ignore_rows: [default 0]
    ini: [default None]
    settings: for MCSamples(settings=)
    '''
    info = yaml_load_file(root+'__full.yaml')
    if not hasattr(info, "keys"):
        raise TypeError("Cannot regonise arguments. Are you sure you are calling "
                        "with (info, collections, ...) in that order?")
    #=============================================
    # read in header names
    hnames = pd.read_csv(root+'_1.txt', header=None, 
                    sep='\s+', nrows=1, low_memory=False)
    num = hnames.columns[-1]
    # the columns names
    columns = ['']*num
    for i in range(1,num+1):
        columns[i-1] = hnames[i][0]
    #-------------------------------------------
    # read in the data
    sp = root.rsplit(sep='/')[-1]
    isp = len(sp)+1
    dirs = root.rstrip(sp)
    test_dat = pd.read_csv(root+'_1.txt', header=None, names=columns, sep='\s+', low_memory=False)
    burn_rows = np.int64(test_dat.shape[0]*burns)
    dats = pd.DataFrame()
    for ifile in os.listdir(dirs):
        if(ifile[-4:] == '.txt' and ifile[:isp] == sp+'_'):
            print(dirs+ifile)
            dat = pd.read_csv(dirs+ifile, header=None, skiprows=burn_rows+1, names=columns, sep='\s+', low_memory=False)
            dats = dats.append(dat, ignore_index=True)
    pp = dats
    #==================================
    ve = getdist.__version__
    if(int(ve[0]) < 1 and int(ve[2]) <= 3 and int(ve[4]) <=3):
        from getdist.yaml_format_tools import _p_label, _p_renames, _weight, _minuslogpost
        from getdist.yaml_format_tools import get_info_params, get_range, is_derived_param
        from getdist.yaml_format_tools import get_sampler_type
    else:
        from getdist.cobaya_interface import _p_label, _p_renames, _weight, _minuslogpost
        from getdist.cobaya_interface import get_info_params, get_range, is_derived_param
        from getdist.cobaya_interface import get_sampler_type
    #==================================
    # Check consistency with info
    info_params = get_info_params(info)
    assert set(columns[2:]) == set(info_params.keys()), (
            "Info and collection(s) are not compatible, because their parameters differ: "
            "the collection(s) have %r and the info has %r. " % (
                columns[2:], list(info_params.keys())) +
            "Are you sure that you are using an *updated* info dictionary "
            "(i.e. the output of `cobaya.run`)?")
    # We need to use *collection* sorting, not info sorting!
    names = [p + ("*" if is_derived_param(info_params[p]) else "")
             for p in columns[2:]]
    labels = [(info_params[p] or {}).get(_p_label, p) for p in columns[2:]]
    ranges = {p: get_range(info_params[p]) for p in columns[2:]}
    renames = {p: info_params.get(p, {}).get(_p_renames, []) for p in columns[2:]}
    
    samples = [np.array([pp[nn].values for nn in columns[2:]]).T]
    get_weights = [pp[nn].values for nn in columns[0:1]]
    if(re_weight is None):
        weights = get_weights
    else:
        scale = np.min(re_weight)
        reweight_loglike = re_weight - scale
        weights = np.int64( get_weights*np.exp(-reweight_loglike) )
    loglikes = [-pp[nn].values for nn in columns[1:2]]
    sampler = get_sampler_type(info)
    
    return MCSamples(samples=samples, weights=weights, loglikes=loglikes, sampler=sampler,
                     names=names, labels=labels, ranges=ranges, renames=renames,
                     ignore_rows=ignore_rows, name_tag=name_tag, ini=ini,
                     settings=settings)
    #return np.shape(samples)

